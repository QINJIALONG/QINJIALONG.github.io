<div style="text-align: justify;">


---
permalink: /
title: "Welcome to Jialong Qin's personal page!"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


![d76fb7aee2eb4322e06545f35f71293](https://github.com/user-attachments/assets/9f0c961c-1781-4780-b067-49c8fed55e86)

My name is **Qin Jialong** and you can call me **Roya**! You can find my CV here: [Roya's Curriculum Vitae](../assets/Curriculum_Vitae.pdf).

I'm a **junior** undergraduate student from School of computer science in [Beijing Institute of Technology](https://bit.edu.cn/) majoring in data science. My research interests focus on large language models, multi-modal data learning, and recommendation systems. Meanwhile, I am also interested in reinforcement learning.

During my sophomore year, I was interested in mathematical modeling with a published **EI-indexed paper** during the Amarican Mathematical Contest in Modeling (MCM). Meanwhile, I got the **first prize** of the China Undergraduate Mathematical Contest in Modeling (CUMCM) in Beijing. While learning data modeling, I’ve found great interest in working with large datasets and algorithms. I’ve picked up many mathematical methods, which have helped me develop careful thinking and strong problem-solving skills. This has set a solid foundation for my future research and studies.

In my junior year, I've been an intern in the lab of BIT under Hu Linmei focusing on natural language processing and lab of Chinese Academy of Sciences under Bi Keping focusing on information retrival. I learned the basic framework of [Transformers](https://arxiv.org/abs/1706.03762) and [BERT](https://arxiv.org/abs/1810.04805). My first demo work is to help a research project called Graph Gravitation Network in Hu's lab, focusing on graph learning. Afterwards, I fine-tuned the  bert model with SQuAD dataset in Bi's lab. Then, I studied the Knowledge Graph embedding models like TransE and DistMult and recommendation systems(RS), especial sequential RS like Gru4Rec,SAS4Rec,CL4Rec,KDA etc. During my two internship experiences, I did some practical data-driven pytorch experiments and gradually identified my research focus as **data mining**. At the same time, I have been closely following and am highly optimistic about the future in **reinforcement learning** and **artificial intelligence**, particularly **LLMs/VLMs**. I am especially interested in the subdirection of **multimodal data, recommendation systems, and reinforcement learning**.

![Graph Gravitation Network](https://github.com/user-attachments/assets/83b5259d-1a90-4d6c-ad31-9c2cd4d735d3)

Currently, I have undertaken two independent research projects. One focuses on integrating knowledge graphs with large models for recommendation systems, and the other explores the fine-tuning and training of vision-language large models. The computational resources for these projects are primarily supported by the laboratory of Prof. Bi Keping at the Institute of Computing Technology, Chinese Academy of Sciences.

## Project 1: Intended for data mining journal/conference
### Independent Research: Enhancing Sequential Recommendation Systems utilizing LLMs and Knowledge Graphs  
- First paper that utilizes self-supervised methods to predict latent item relationships in dynamic knowledge graphs based on LLM for relation-aware sequential recommender systems.  
- Harnesses the language knowledge to discover latent relations and flexible to work with existing relation-aware sequential recommenders through joint learning.  
- Significantly improves the performance of existing relation-aware sequential recommendation models and achieve state of the art performance on real world public recommendation data set.

![CORE work](https://github.com/user-attachments/assets/b283ac09-a1b7-47c2-a293-fd4a73ad99c2)

## Project 2
### WWW2025 Multi-modal Dialogue System Intent Recognition Challenge
- Fine-tuned VLM Qwen2-vl-7B using 4*A800(80G) GPUs and achieved an 8% improvement above baselines successfully.  
- Data Augmentation: Infuse prior knowledge into the dataset to enhance performance in data-sparse scenarios.  
- Self-Consistency: Employ multiple reasoning paths to ensure the correctness of the answers.  
- Pretraining: Utilize pseudo-labeling to introduces a wealth of domain knowledge while applying gradient clipping to mitigate the adverse effects of noisy labels on the training process.  
- Intent Summarization: Summarize user-specific intents and filter out noise to refine the input.

# Papers

## CORE: Contrastive Optimized with Relation-aware Embeddings for Sequential Recommendation
This is exact the project 1, which has not been published yet and is intended for data mining journal/conference

## Co-First Author: A Dynamic Analysis Approach in Racket Sports. SPCS. [EI conference]  
- Employed ARIMA, TOPSIS method and Lasso regression to quantify and predict momentum disparities among players.  
- Analyzed the momentum fluctuation of players focusing on the 2023 Wimbledon Men's Singles and other events.

![image](https://github.com/user-attachments/assets/add2453d-689c-4812-822b-4a053c25ced7)

# SKILLS & INTERESTS

- **Languages**: English(fluent) Chinese(native) I am preparaing for taking IELTs recently!
- **Programming**:
  1. Proficient in Python. (I have handled several python projects by using pytorch)
  2. Skilled in C++ for solving problems on LeetCode(similar to codeforce.com).(In this process, I strengthed my programming skills and understanding of data structures and algorithms.)
  3. Java for web development (I have developed a content sharing website by deploying a Spring Boot-based back-end and VUE3-based front-end with instant messaging, content publishing and interactive features.)
- **Volunteer Experience**: Accumulated over 100 hours of volunteer work. Volunteered 50 hours providing free compute repair services to students at Beijing Institute of Technology Network Pioneers Association Computer Clinic.

</div>
